{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ad2149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.0.1\n",
      "  Downloading torch-2.0.1-cp39-cp39-win_amd64.whl (172.4 MB)\n",
      "Collecting transformers==4.32.0\n",
      "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
      "Collecting peft==0.5.0\n",
      "  Downloading peft-0.5.0-py3-none-any.whl (85 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.0-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "Collecting yfinance\n",
      "  Downloading yfinance-0.2.36-py2.py3-none-any.whl (72 kB)\n",
      "Collecting finnhub-python\n",
      "  Downloading finnhub_python-2.4.19-py3-none-any.whl (11 kB)\n",
      "Collecting nvidia-ml-py3\n",
      "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.9.0-py3-none-any.whl (223 kB)\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\0. milindpython\\fingpt\\fingpt-1\\.venv\\lib\\site-packages (from torch==2.0.1->-r requirements.txt (line 1)) (4.9.0)\n",
      "Collecting jinja2\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.15.1\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.1-cp39-none-win_amd64.whl (277 kB)\n",
      "Collecting numpy>=1.17\n",
      "  Downloading numpy-1.26.3-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.12.25-cp39-cp39-win_amd64.whl (269 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\0. milindpython\\fingpt\\fingpt-1\\.venv\\lib\\site-packages (from transformers==4.32.0->-r requirements.txt (line 2)) (23.2)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-win_amd64.whl (152 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
      "Requirement already satisfied: psutil in c:\\0. milindpython\\fingpt\\fingpt-1\\.venv\\lib\\site-packages (from peft==0.5.0->-r requirements.txt (line 3)) (5.9.8)\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\0. milindpython\\fingpt\\fingpt-1\\.venv\\lib\\site-packages (from pandas->-r requirements.txt (line 4)) (2.8.2)\n",
      "Collecting beautifulsoup4>=4.11.1\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Collecting appdirs>=1.4.4\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting lxml>=4.9.1\n",
      "  Downloading lxml-5.1.0-cp39-cp39-win_amd64.whl (3.9 MB)\n",
      "Collecting html5lib>=1.1\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Collecting frozendict>=2.3.4\n",
      "  Downloading frozendict-2.4.0-cp39-cp39-win_amd64.whl (36 kB)\n",
      "Collecting peewee>=3.16.2\n",
      "  Downloading peewee-3.17.0.tar.gz (2.9 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting multitasking>=0.0.7\n",
      "  Downloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Collecting sniffio\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "Collecting anyio<5,>=3.5.0\n",
      "  Downloading anyio-4.2.0-py3-none-any.whl (85 kB)\n",
      "Collecting pydantic<3,>=1.9.0\n",
      "  Downloading pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.4-cp39-cp39-win_amd64.whl (17 kB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp39-cp39-win_amd64.whl (100 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\0. milindpython\\fingpt\\fingpt-1\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers==4.32.0->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\0. milindpython\\fingpt\\fingpt-1\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 4)) (1.16.0)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
      "Collecting webencodings\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2; python_version < \"3.11\" in c:\\0. milindpython\\fingpt\\fingpt-1\\.venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 8)) (1.2.0)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting pydantic-core==2.14.6\n",
      "  Downloading pydantic_core-2.14.6-cp39-none-win_amd64.whl (1.9 MB)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using legacy 'setup.py install' for nvidia-ml-py3, since package 'wheel' is not installed.\n",
      "Building wheels for collected packages: peewee\n",
      "  Building wheel for peewee (PEP 517): started\n",
      "  Building wheel for peewee (PEP 517): finished with status 'done'\n",
      "  Created wheel for peewee: filename=peewee-3.17.0-py3-none-any.whl size=135766 sha256=7fd41b8e02722a33351e8783cd2c591f3f03907524daea42db4a667e359221cd\n",
      "  Stored in directory: c:\\users\\mpari\\appdata\\local\\pip\\cache\\wheels\\77\\6b\\dc\\722aee2a5d46a021ab67564c9d3d5536c3bd9577fcdc3a5aeb\n",
      "Successfully built peewee\n",
      "Installing collected packages: mpmath, sympy, MarkupSafe, jinja2, networkx, filelock, torch, idna, certifi, charset-normalizer, urllib3, requests, fsspec, tqdm, pyyaml, huggingface-hub, safetensors, numpy, regex, tokenizers, transformers, accelerate, peft, tzdata, pytz, pandas, soupsieve, beautifulsoup4, appdirs, lxml, webencodings, html5lib, frozendict, peewee, multitasking, yfinance, finnhub-python, nvidia-ml-py3, sniffio, distro, h11, httpcore, anyio, httpx, annotated-types, pydantic-core, pydantic, openai\n",
      "    Running setup.py install for nvidia-ml-py3: started\n",
      "    Running setup.py install for nvidia-ml-py3: finished with status 'done'\n",
      "Successfully installed MarkupSafe-2.1.4 accelerate-0.26.1 annotated-types-0.6.0 anyio-4.2.0 appdirs-1.4.4 beautifulsoup4-4.12.3 certifi-2023.11.17 charset-normalizer-3.3.2 distro-1.9.0 filelock-3.13.1 finnhub-python-2.4.19 frozendict-2.4.0 fsspec-2023.12.2 h11-0.14.0 html5lib-1.1 httpcore-1.0.2 httpx-0.26.0 huggingface-hub-0.20.3 idna-3.6 jinja2-3.1.3 lxml-5.1.0 mpmath-1.3.0 multitasking-0.0.11 networkx-3.2.1 numpy-1.26.3 nvidia-ml-py3-7.352.0 openai-1.9.0 pandas-2.2.0 peewee-3.17.0 peft-0.5.0 pydantic-2.5.3 pydantic-core-2.14.6 pytz-2023.3.post1 pyyaml-6.0.1 regex-2023.12.25 requests-2.31.0 safetensors-0.4.1 sniffio-1.3.0 soupsieve-2.5 sympy-1.12 tokenizers-0.13.3 torch-2.0.1 tqdm-4.66.1 transformers-4.32.0 tzdata-2023.4 urllib3-2.1.0 webencodings-0.5.1 yfinance-0.2.36\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.3; however, version 23.3.2 is available.\n",
      "You should consider upgrading via the 'c:\\0. MilindPython\\FinGPT\\FinGPT-1\\.venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c4d096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import finnhub\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from datasets import Dataset\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ace9fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#START_DATE = input(\"Enter Start Date to get data: (YYYY-MM-DD)\")\n",
    "START_DATE = \"2022-12-31\"\n",
    "#END_DATE = \"2023-05-31\"\n",
    "#END_DATE = input(\"Enter End Date to get data: (YYYY-MM-DD)\")\n",
    "END_DATE = \"2024-01-18\"\n",
    "\n",
    "DATA_DIR = f\"./{START_DATE}_{END_DATE}\"\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ead8c30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cmlaql9r01qmnetgnp7gcmlaql9r01qmnetgnp80\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import sys\n",
    "\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_= load_dotenv(find_dotenv()) #read local .env file\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai_api_key = openai.api_key\n",
    "print(openai_api_key)\n",
    "finnhub_api_key = os.environ['FINNHUB_IO_API_KEY']\n",
    "print(finnhub_api_key)\n",
    "\n",
    "finnhub_client = finnhub.Client(api_key=finnhub_api_key)\n",
    "client = OpenAI(api_key = openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fce2503",
   "metadata": {},
   "source": [
    "# Raw Financial Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6564114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_mapping(ret):\n",
    "    \n",
    "    up_down = 'U' if ret >= 0 else 'D'\n",
    "    integer = math.ceil(abs(100 * ret))\n",
    "    \n",
    "    return up_down + (str(integer) if integer <= 5 else '5+')\n",
    "\n",
    "\n",
    "def get_returns(stock_symbol):\n",
    "    \n",
    "    # Download historical stock data\n",
    "    print(stock_symbol, ': ', START_DATE, ' - ', END_DATE)\n",
    "    stock_data = yf.download(stock_symbol, start=START_DATE, end=END_DATE)\n",
    "    \n",
    "    weekly_data = stock_data['Adj Close'].resample('W').ffill()\n",
    "    weekly_returns = weekly_data.pct_change()[1:]\n",
    "    weekly_start_prices = weekly_data[:-1]\n",
    "    weekly_end_prices = weekly_data[1:]\n",
    "\n",
    "    weekly_data = pd.DataFrame({\n",
    "        'Start Date': weekly_start_prices.index,\n",
    "        'Start Price': weekly_start_prices.values,\n",
    "        'End Date': weekly_end_prices.index,\n",
    "        'End Price': weekly_end_prices.values,\n",
    "        'Weekly Returns': weekly_returns.values\n",
    "    })\n",
    "    \n",
    "    weekly_data['Bin Label'] = weekly_data['Weekly Returns'].map(bin_mapping)\n",
    "\n",
    "    return weekly_data\n",
    "\n",
    "\n",
    "def get_news(symbol, data):\n",
    "    \n",
    "    news_list = []\n",
    "    \n",
    "    for end_date, row in data.iterrows():\n",
    "        start_date = row['Start Date'].strftime('%Y-%m-%d')\n",
    "        end_date = row['End Date'].strftime('%Y-%m-%d')\n",
    "        print(symbol, ': ', start_date, ' - ', end_date)\n",
    "        time.sleep(1) # control qpm\n",
    "        weekly_news = finnhub_client.company_news(symbol, _from=start_date, to=end_date)\n",
    "        if weekly_news != None:\n",
    "            formatted_date = None  # Initialize 'formatted_date' outside the try block\n",
    "            weekly_news_result = [{\n",
    "                \"date\": datetime.fromtimestamp(n['datetime']).strftime('%Y%m%d%H%M%S'),\n",
    "                \"headline\": n['headline'],\n",
    "                \"summary\": n['summary'],\n",
    "                } for n in weekly_news\n",
    "                \n",
    "                if (lambda x: x.strftime('%Y%m%d%H%M%S') if isinstance(x, datetime) else None)(formatted_date)\n",
    "                ]\n",
    "\n",
    "        weekly_news.sort(key=lambda x: x.get('date', ''))\n",
    "        news_list.append(json.dumps(weekly_news))\n",
    "    \n",
    "    data['News'] = news_list\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_basics(symbol, data, always=False):\n",
    "    \n",
    "    basic_financials = finnhub_client.company_basic_financials(symbol, 'all')\n",
    "    \n",
    "    final_basics, basic_list, basic_dict = [], [], defaultdict(dict)\n",
    "    \n",
    "    for metric, value_list in basic_financials['series']['quarterly'].items():\n",
    "        for value in value_list:\n",
    "            basic_dict[value['period']].update({metric: value['v']})\n",
    "\n",
    "    for k, v in basic_dict.items():\n",
    "        v.update({'period': k})\n",
    "        basic_list.append(v)\n",
    "        \n",
    "    basic_list.sort(key=lambda x: x['period'])\n",
    "            \n",
    "    for i, row in data.iterrows():\n",
    "        \n",
    "        start_date = row['End Date'].strftime('%Y-%m-%d')\n",
    "        last_start_date = START_DATE if i < 2 else data.loc[i-2, 'Start Date'].strftime('%Y-%m-%d')\n",
    "        \n",
    "        used_basic = {}\n",
    "        for basic in basic_list[::-1]:\n",
    "            if (always and basic['period'] < start_date) or (last_start_date <= basic['period'] < start_date):\n",
    "                used_basic = basic\n",
    "                break\n",
    "        final_basics.append(json.dumps(used_basic))\n",
    "        \n",
    "    data['Basics'] = final_basics\n",
    "    \n",
    "    return data\n",
    "    \n",
    "\n",
    "def prepare_data_for_company(symbol, indice, with_basics=True):\n",
    "    \n",
    "    print(\"Getting returns...\")\n",
    "    data = get_returns(symbol)\n",
    "    print(\"Getting news...\")\n",
    "    data = get_news(symbol, data)\n",
    "    \n",
    "    if with_basics:\n",
    "        print(\"Getting basics...\")\n",
    "        data = get_basics(symbol, data)\n",
    "        data.to_csv(f\"{DATA_DIR}/{indice}_{symbol}_{START_DATE}_{END_DATE}.csv\")\n",
    "    else:\n",
    "        data['Basics'] = [json.dumps({})] * len(data)\n",
    "        data.to_csv(f\"{DATA_DIR}/{indice}_{symbol}_{START_DATE}_{END_DATE}_nobasics.csv\")\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caf02ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOW_30 = [\n",
    "    \"AXP\", \"AMGN\", \"AAPL\", \"BA\", \"CAT\", \"CSCO\", \"CVX\", \"GS\", \"HD\", \"HON\",\n",
    "    \"IBM\", \"INTC\", \"JNJ\", \"KO\", \"JPM\", \"MCD\", \"MMM\", \"MRK\", \"MSFT\", \"NKE\",\n",
    "    \"PG\", \"TRV\", \"UNH\", \"CRM\", \"VZ\", \"V\", \"WBA\", \"WMT\", \"DIS\", \"DOW\"\n",
    "]\n",
    "\n",
    "NASDAQ_100 = [\n",
    "    \"MSFT\",\"AAPL\",\"AMZN\",\"NVDA\",\"AVGO\",\"META\",\"TSLA\",\"GOOGL\",\"GOOG\",\"COST\",\n",
    "    \"ADBE\",\"AMD\",\"PEP\",\"NFLX\",\"CSCO\",\"INTC\",\"TMUS\",\"CMCSA\",\"INTU\",\"AMGN\",\n",
    "    \"QCOM\",\"TXN\",\"HON\",\"AMAT\",\"ISRG\",\"BKNG\",\"VRTX\",\"GILD\",\"SBUX\",\"PANW\",\n",
    "    \"MDLZ\",\"REGN\",\"LRCX\",\"ADP\",\"PDD\",\"ADI\",\"MU\",\"MELI\",\"SNPS\",\"KLAC\",\"CDNS\",\n",
    "    \"CSX\",\"MAR\",\"PYPL\",\"CRWD\",\"ASML\",\"CTAS\",\"MNST\",\"WDAY\",\"ORLY\",\"ABNB\",\"ROP\",\n",
    "    \"LULU\",\"MRVL\",\"CHTR\",\"NXPI\",\"ADSK\",\"PCAR\",\"DXCM\",\"FTNT\",\"KHC\",\"ROST\",\"CPRT\",\n",
    "    \"MCHP\",\"KDP\",\"PAYX\",\"IDXX\",\"AEP\",\"ODFL\",\"AZN\",\"MRNA\",\"DASH\",\"DDOG\",\"CTSH\",\"EA\",\n",
    "    \"TEAM\",\"FAST\",\"CEG\",\"BIIB\",\"EXC\",\"VRSK\",\"ZS\",\"CSGP\",\"XEL\",\"GEHC\",\"ON\",\"BKR\",\"CCEP\",\n",
    "    \"GFS\",\"DLTR\",\"CDW\",\"TTD\",\"ANSS\",\"MDB\",\"TTWO\",\"FANG\",\"SPLK\",\"WBD\",\"ILMN\",\"SIRI\",\"WBA\",\n",
    "    ]\n",
    "\n",
    "# prepare_data_for_company(\"DOW\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d65960",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "for symbol in NASDAQ_100:\n",
    "    prepare_data_for_company(symbol,'NASDAQ_100')\n",
    "\n",
    "for symbol in DOW_30:\n",
    "    prepare_data_for_company(symbol, 'DOW_30')\n",
    "#     prepare_data_for_company(symbol, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af655d8b",
   "metadata": {},
   "source": [
    "# Generate Prompt from Financial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a53c0ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_company_prompt(symbol):\n",
    "    \n",
    "    profile = finnhub_client.company_profile2(symbol=symbol)\n",
    "\n",
    "    company_template = \"[Company Introduction]:\\n\\n{name} is a leading entity in the {finnhubIndustry} sector. Incorporated and publicly traded since {ipo}, the company has established its reputation as one of the key players in the market. As of today, {name} has a market capitalization of {marketCapitalization:.2f} in {currency}, with {shareOutstanding:.2f} shares outstanding.\" \\\n",
    "        \"\\n\\n{name} operates primarily in the {country}, trading under the ticker {ticker} on the {exchange}. As a dominant force in the {finnhubIndustry} space, the company continues to innovate and drive progress within the industry.\"\n",
    "\n",
    "    formatted_str = company_template.format(**profile)\n",
    "    \n",
    "    return formatted_str\n",
    "\n",
    "\n",
    "def get_prompt_by_row(symbol, row):\n",
    "\n",
    "    start_date = row['Start Date'] if isinstance(row['Start Date'], str) else row['Start Date'].strftime('%Y-%m-%d')\n",
    "    end_date = row['End Date'] if isinstance(row['End Date'], str) else row['End Date'].strftime('%Y-%m-%d')\n",
    "    term = 'increased' if row['End Price'] > row['Start Price'] else 'decreased'\n",
    "    head = \"From {} to {}, {}'s stock price {} from {:.2f} to {:.2f}. Company news during this period are listed below:\\n\\n\".format(\n",
    "        start_date, end_date, symbol, term, row['Start Price'], row['End Price'])\n",
    "    \n",
    "    news = json.loads(row[\"News\"])\n",
    "    news = [\"[Headline]: {}\\n[Summary]: {}\\n\".format(\n",
    "        n['headline'], n['summary']) for n in news if n['date'][:8] <= end_date.replace('-', '') and \\\n",
    "        not n['summary'].startswith(\"Looking for stock market analysis and research with proves results?\")]\n",
    "\n",
    "    basics = json.loads(row['Basics'])\n",
    "    if basics:\n",
    "        basics = \"Some recent basic financials of {}, reported at {}, are presented below:\\n\\n[Basic Financials]:\\n\\n\".format(\n",
    "            symbol, basics['period']) + \"\\n\".join(f\"{k}: {v}\" for k, v in basics.items() if k != 'period')\n",
    "    else:\n",
    "        basics = \"[Basic Financials]:\\n\\nNo basic financial reported.\"\n",
    "    \n",
    "    return head, news, basics\n",
    "\n",
    "\n",
    "def sample_news(news, k=5):\n",
    "    \n",
    "    return [news[i] for i in sorted(random.sample(range(len(news)), k))]\n",
    "\n",
    "\n",
    "def map_bin_label(bin_lb):\n",
    "    \n",
    "    lb = bin_lb.replace('U', 'up by ')\n",
    "    lb = lb.replace('D', 'down by ')\n",
    "    lb = lb.replace('1', '0-1%')\n",
    "    lb = lb.replace('2', '1-2%')\n",
    "    lb = lb.replace('3', '2-3%')\n",
    "    lb = lb.replace('4', '3-4%')\n",
    "    if lb.endswith('+'):\n",
    "        lb = lb.replace('5+', 'more than 5%')\n",
    "#         lb = lb.replace('5+', '5+%')\n",
    "    else:\n",
    "        lb = lb.replace('5', '4-5%')\n",
    "    \n",
    "    return lb\n",
    "\n",
    "\n",
    "def get_all_prompts(symbol, indice, min_past_weeks=1, max_past_weeks=3, with_basics=True):\n",
    "\n",
    "    \n",
    "    if with_basics:\n",
    "        try:\n",
    "            indice=\"DOW_30\"\n",
    "            df = pd.read_csv(f'{DATA_DIR}/{indice}_{symbol}_{START_DATE}_{END_DATE}.csv')\n",
    "        except:\n",
    "            indice=\"NASDAQ_100\"            \n",
    "            df = pd.read_csv(f'{DATA_DIR}/{indice}_{symbol}_{START_DATE}_{END_DATE}.csv')\n",
    "    else:\n",
    "        try:\n",
    "            indice = \"DOW_30\"\n",
    "            df = pd.read_csv(f'{DATA_DIR}/{indice}_{symbol}_{START_DATE}_{END_DATE}_nobasics.csv')\n",
    "        except:\n",
    "            indice = \"NASDAQ_100\"\n",
    "            df = pd.read_csv(f'{DATA_DIR}/{indice}_{symbol}_{START_DATE}_{END_DATE}_nobasics.csv')\n",
    "        \n",
    "    company_prompt = get_company_prompt(symbol)\n",
    "\n",
    "    prev_rows = []\n",
    "    all_prompts = []\n",
    "\n",
    "    for row_idx, row in df.iterrows():\n",
    "\n",
    "        prompt = \"\"\n",
    "        if len(prev_rows) >= min_past_weeks:\n",
    "            idx = min(random.choice(range(min_past_weeks, max_past_weeks+1)), len(prev_rows))\n",
    "            for i in range(-idx, 0):\n",
    "                # Add Price Movement (Head)\n",
    "                prompt += \"\\n\" + prev_rows[i][0]\n",
    "                # Add News of previous weeks\n",
    "                sampled_news = sample_news(\n",
    "                    prev_rows[i][1],\n",
    "                    min(5, len(prev_rows[i][1]))\n",
    "                )\n",
    "                if sampled_news:\n",
    "                    prompt += \"\\n\".join(sampled_news)\n",
    "                else:\n",
    "                    prompt += \"No relative news reported.\"\n",
    "\n",
    "        head, news, basics = get_prompt_by_row(symbol, row)\n",
    "\n",
    "        prev_rows.append((head, news, basics))\n",
    "        if len(prev_rows) > max_past_weeks:\n",
    "            prev_rows.pop(0)  \n",
    "\n",
    "        if not prompt:\n",
    "            continue\n",
    "\n",
    "        prediction = map_bin_label(row['Bin Label'])\n",
    "        \n",
    "        prompt = company_prompt + '\\n' + prompt + '\\n' + basics\n",
    "        prompt += f\"\\n\\nBased on all the information before {row['Start Date']}, let's first analyze the positive developments and potential concerns for {symbol}. Come up with 2-4 most important factors respectively and keep them concise. Most factors should be inferred from company related news. \" \\\n",
    "            f\"Then let's assume your prediction for next week ({row['Start Date']} to {row['End Date']}) is {prediction}. Provide a summary analysis to support your prediction. The prediction result need to be inferred from your analysis at the end, and thus not appearing as a foundational factor of your analysis.\"\n",
    "\n",
    "        all_prompts.append(prompt.strip())\n",
    "    \n",
    "    return all_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92208b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a seasoned stock market analyst. Your task is to list the positive developments and potential concerns for companies based on relevant news and basic financials from the past weeks, then provide an analysis and prediction for the companies' stock price movement for the upcoming week. Your answer format should be as follows:\n",
      "\n",
      "[Positive Developments]:\n",
      "1. ...\n",
      "\n",
      "[Potential Concerns]:\n",
      "1. ...\n",
      "\n",
      "[Prediction & Analysis]:\n",
      "...\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(SYSTEM_PROMPT)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# prompts = get_all_prompts(\"AAPL\", 1, 3)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# prompts = get_all_prompts(\"MSFT\", 1, 3, False)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m prompts \u001b[38;5;241m=\u001b[39m \u001b[43mget_all_prompts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTRV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(prompts[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[8], line 99\u001b[0m, in \u001b[0;36mget_all_prompts\u001b[1;34m(symbol, indice, min_past_weeks, max_past_weeks, with_basics)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m             prompt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo relative news reported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 99\u001b[0m head, news, basics \u001b[38;5;241m=\u001b[39m \u001b[43mget_prompt_by_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m prev_rows\u001b[38;5;241m.\u001b[39mappend((head, news, basics))\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prev_rows) \u001b[38;5;241m>\u001b[39m max_past_weeks:\n",
      "Cell \u001b[1;32mIn[8], line 22\u001b[0m, in \u001b[0;36mget_prompt_by_row\u001b[1;34m(symbol, row)\u001b[0m\n\u001b[0;32m     18\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrom \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms stock price \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m. Company news during this period are listed below:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     19\u001b[0m     start_date, end_date, symbol, term, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart Price\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnd Price\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     21\u001b[0m news \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNews\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 22\u001b[0m news \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Headline]: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[Summary]: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     23\u001b[0m     n[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheadline\u001b[39m\u001b[38;5;124m'\u001b[39m], n[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m news \u001b[38;5;28;01mif\u001b[39;00m n[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m8\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m end_date\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m n[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLooking for stock market analysis and research with proves results?\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     26\u001b[0m basics \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBasics\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m basics:\n",
      "Cell \u001b[1;32mIn[8], line 23\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     18\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrom \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms stock price \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m. Company news during this period are listed below:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     19\u001b[0m     start_date, end_date, symbol, term, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart Price\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnd Price\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     21\u001b[0m news \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNews\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     22\u001b[0m news \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Headline]: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[Summary]: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m---> 23\u001b[0m     n[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheadline\u001b[39m\u001b[38;5;124m'\u001b[39m], n[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m news \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mn\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[:\u001b[38;5;241m8\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m end_date\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m n[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLooking for stock market analysis and research with proves results?\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     26\u001b[0m basics \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBasics\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m basics:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a seasoned stock market analyst. Your task is to list the positive developments and potential concerns for companies based on relevant news and basic financials from the past weeks, then provide an analysis and prediction for the companies' stock price movement for the upcoming week. \" \\\n",
    "    \"Your answer format should be as follows:\\n\\n[Positive Developments]:\\n1. ...\\n\\n[Potential Concerns]:\\n1. ...\\n\\n[Prediction & Analysis]:\\n...\\n\"\n",
    "\n",
    "print(SYSTEM_PROMPT)\n",
    "\n",
    "# prompts = get_all_prompts(\"AAPL\", 1, 3)\n",
    "# prompts = get_all_prompts(\"MSFT\", 1, 3, False)\n",
    "prompts = get_all_prompts(\"TRV\", 1, 4)\n",
    "\n",
    "print(prompts[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b010a45",
   "metadata": {},
   "source": [
    "# Request to GPT-4 for Financial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e355117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(filename, input_data, output_data):\n",
    "    \n",
    "    with open(filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([input_data, output_data])\n",
    "\n",
    "        \n",
    "def initialize_csv(filename):\n",
    "    \n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"prompt\", \"answer\"])\n",
    "\n",
    "\n",
    "def query_gpt35(symbol_list, min_past_weeks=1, max_past_weeks=3, with_basics=True):\n",
    "\n",
    "    for symbol in symbol_list:\n",
    "        \n",
    "        csv_file = f'{DATA_DIR}/{symbol}_{START_DATE}_{END_DATE}_gpt-35.csv' if with_basics else \\\n",
    "                   f'{DATA_DIR}/{symbol}_{START_DATE}_{END_DATE}_nobasics_gpt-35.csv'\n",
    "        \n",
    "        if not os.path.exists(csv_file):\n",
    "            initialize_csv(csv_file)\n",
    "            pre_done = 0\n",
    "        else:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            pre_done = len(df)\n",
    "\n",
    "        prompts = get_all_prompts(symbol, min_past_weeks, max_past_weeks, with_basics)\n",
    "\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            \n",
    "            if i < pre_done:\n",
    "                continue\n",
    "\n",
    "            print(f\"{symbol} - {i}\")\n",
    "            \n",
    "            cnt = 0\n",
    "            while cnt < 5:\n",
    "                try:\n",
    "                    completion = client.chat.completions.create(\n",
    "                        model=\"gpt-3.5-turbo\",\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                          ]\n",
    "                    )\n",
    "                    break    \n",
    "                except Exception:\n",
    "                    cnt += 1\n",
    "                    print(f'retry cnt {cnt}')\n",
    "            \n",
    "            answer = completion.choices[0].message.content if cnt < 5 else \"\"\n",
    "            append_to_csv(csv_file, prompt, answer)\n",
    "      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def query_gpt4(symbol_list, min_past_weeks=1, max_past_weeks=3, with_basics=True):\n",
    "\n",
    "    for symbol in symbol_list:\n",
    "        \n",
    "        csv_file = f'{DATA_DIR}/{symbol}_{START_DATE}_{END_DATE}_gpt-4.csv' if with_basics else \\\n",
    "                   f'{DATA_DIR}/{symbol}_{START_DATE}_{END_DATE}_nobasics_gpt-4.csv'\n",
    "        \n",
    "        if not os.path.exists(csv_file):\n",
    "            initialize_csv(csv_file)\n",
    "            pre_done = 0\n",
    "        else:\n",
    "            df = pd.read_csv(csv_file)\n",
    "            pre_done = len(df)\n",
    "\n",
    "        prompts = get_all_prompts(symbol, min_past_weeks, max_past_weeks, with_basics)\n",
    "\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            \n",
    "            if i < pre_done:\n",
    "                continue\n",
    "\n",
    "            print(f\"{symbol} - {i}\")\n",
    "            \n",
    "            cnt = 0\n",
    "            while cnt < 5:\n",
    "                try:\n",
    "                    completion = client.chat.completions.create(\n",
    "                        model=\"gpt-4\",\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                            {\"role\": \"user\", \"content\": prompt}\n",
    "                          ]\n",
    "                    )\n",
    "                    break    \n",
    "                except Exception:\n",
    "                    cnt += 1\n",
    "                    print(f'retry cnt {cnt}')\n",
    "            \n",
    "            answer = completion.choices[0].message.content if cnt < 5 else \"\"\n",
    "            append_to_csv(csv_file, prompt, answer)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ff6ff3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# query_gpt4(DOW_30, 1, 3)\n",
    "query_gpt4(DOW_30, 1, 4)\n",
    "#query_gpt35(DOW_30, 1, 4)\n",
    "query_gpt35(['TRV'], 1, 4)\n",
    "# query_gpt4(['WBA'], 1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238ba9f0",
   "metadata": {},
   "source": [
    "# Transform into Llama2 Training Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2627f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt4_to_llama(symbol, with_basics=True):\n",
    "    \n",
    "    csv_file = f'{DATA_DIR}/{symbol}_{START_DATE}_{END_DATE}_gpt-4.csv' if with_basics else \\\n",
    "                   f'{DATA_DIR}/{symbol}_{START_DATE}_{END_DATE}_nobasics_gpt-4.csv'\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    prompts, answers, periods, labels = [], [], [], []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        \n",
    "        prompt, answer = row['prompt'], row['answer']\n",
    "        \n",
    "        res = re.search(r\"Then let's assume your prediction for next week \\((.*)\\) is ((:?up|down) by .*%).\", prompt)\n",
    "        \n",
    "        period, label = res.group(1), res.group(2)\n",
    "#         label = label.replace('more than 5', '5+')\n",
    "        \n",
    "        prompt = re.sub(\n",
    "            r\"Then let's assume your prediction for next week \\((.*)\\) is (up|down) by ((:?.*)%). Provide a summary analysis to support your prediction. The prediction result need to be inferred from your analysis at the end, and thus not appearing as a foundational factor of your analysis.\", \n",
    "            f\"Then make your prediction of the {symbol} stock price movement for next week ({period}). Provide a summary analysis to support your prediction.\",\n",
    "            prompt\n",
    "        )\n",
    "        try:\n",
    "            answer = re.sub(\n",
    "                r\"\\[Prediction & Analysis\\]:\\s*\",\n",
    "                f\"[Prediction & Analysis]:\\nPrediction: {label.capitalize()}\\nAnalysis: \",\n",
    "                answer\n",
    "            )\n",
    "        except Exception:\n",
    "            print(symbol, i)\n",
    "            print(label)\n",
    "            print(answer)\n",
    "            continue\n",
    "            \n",
    "        new_system_prompt = SYSTEM_PROMPT.replace(':\\n...', '\\nPrediction: ...\\nAnalysis: ...')\n",
    "#         new_system_prompt = SYSTEM_PROMPT.replace(':\\n...', '\\nPrediction: {Up|Down} by {1-2|2-3|3-4|4-5|5+}%\\nAnalysis: ...')\n",
    "        \n",
    "        prompt = B_INST + B_SYS + new_system_prompt + E_SYS + prompt + E_INST\n",
    "        \n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "        periods.append(period)\n",
    "        labels.append(label)\n",
    "        \n",
    "    return {\n",
    "        \"prompt\": prompts,\n",
    "        \"answer\": answers,\n",
    "        \"period\": periods,\n",
    "        \"label\": labels,\n",
    "    }\n",
    "\n",
    "\n",
    "def create_dataset(symbol_list, train_ratio=0.8, with_basics=True):\n",
    "\n",
    "    train_dataset_list = []\n",
    "    test_dataset_list = []\n",
    "\n",
    "    for symbol in symbol_list:\n",
    "\n",
    "        data_dict = gpt4_to_llama(symbol, with_basics)\n",
    "#         print(data_dict['prompt'][-1])\n",
    "#         print(data_dict['answer'][-1])\n",
    "        symbols = [symbol] * len(data_dict['label'])\n",
    "        data_dict.update({\"symbol\": symbols})\n",
    "\n",
    "        dataset = Dataset.from_dict(data_dict)\n",
    "        train_size = round(train_ratio * len(dataset))\n",
    "\n",
    "        train_dataset_list.append(dataset.select(range(train_size)))\n",
    "        test_dataset_list.append(dataset.select(range(train_size, len(dataset))))\n",
    "\n",
    "    train_dataset = datasets.concatenate_datasets(train_dataset_list)\n",
    "    test_dataset = datasets.concatenate_datasets(test_dataset_list)\n",
    "\n",
    "    dataset = datasets.DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'test': test_dataset\n",
    "    })\n",
    "    \n",
    "    return dataset\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e089b1bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# v1\n",
    "# dow30_dataset = create_dataset(DOW30, True)\n",
    "# v2\n",
    "# dow30_nobasic_dataset = create_dataset(DOW_30, 0.8, False)\n",
    "# v3\n",
    "dow30_v3_dataset = create_dataset(DOW_30, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123f2db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dow30_dataset.save_to_disk('fingpt-forecaster-dow30-20230601-20230930-llama')\n",
    "# dow30_nobasics_dataset.save_to_disk('fingpt-forecaster-dow30nobasics-20230601-20230930-llama')\n",
    "dow30_v3_dataset.save_to_disk('fingpt-forecaster-dow30v3-20221231-20230531-llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed5cf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dow30_v3_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e711f",
   "metadata": {},
   "source": [
    "# Test-time Information Fetching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292268bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "\n",
    "def get_curday():\n",
    "    \n",
    "    return date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def n_weeks_before(date_string, n):\n",
    "    \n",
    "    date = datetime.strptime(date_string, \"%Y-%m-%d\") - timedelta(days=7*n)\n",
    "    \n",
    "    return date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def get_stock_data(stock_symbol, steps):\n",
    "\n",
    "    stock_data = yf.download(stock_symbol, steps[0], steps[-1])\n",
    "    \n",
    "#     print(stock_data)\n",
    "    \n",
    "    dates, prices = [], []\n",
    "    available_dates = stock_data.index.format()\n",
    "    \n",
    "    for date in steps[:-1]:\n",
    "        for i in range(len(stock_data)):\n",
    "            if available_dates[i] >= date:\n",
    "                prices.append(stock_data['Close'][i])\n",
    "                dates.append(datetime.strptime(available_dates[i], \"%Y-%m-%d\"))\n",
    "                break\n",
    "\n",
    "    dates.append(datetime.strptime(available_dates[-1], \"%Y-%m-%d\"))\n",
    "    prices.append(stock_data['Close'][-1])\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        \"Start Date\": dates[:-1], \"End Date\": dates[1:],\n",
    "        \"Start Price\": prices[:-1], \"End Price\": prices[1:]\n",
    "    })\n",
    "\n",
    "\n",
    "def get_current_basics(symbol, curday):\n",
    "\n",
    "    basic_financials = finnhub_client.company_basic_financials(symbol, 'all')\n",
    "    \n",
    "    final_basics, basic_list, basic_dict = [], [], defaultdict(dict)\n",
    "    \n",
    "    for metric, value_list in basic_financials['series']['quarterly'].items():\n",
    "        for value in value_list:\n",
    "            basic_dict[value['period']].update({metric: value['v']})\n",
    "\n",
    "    for k, v in basic_dict.items():\n",
    "        v.update({'period': k})\n",
    "        basic_list.append(v)\n",
    "        \n",
    "    basic_list.sort(key=lambda x: x['period'])\n",
    "    \n",
    "    for basic in basic_list[::-1]:\n",
    "        if basic['period'] <= curday:\n",
    "            break\n",
    "            \n",
    "    return basic\n",
    "    \n",
    "\n",
    "def get_all_prompts_online(symbol, data, curday, with_basics=True):\n",
    "\n",
    "    company_prompt = get_company_prompt(symbol)\n",
    "\n",
    "    prev_rows = []\n",
    "\n",
    "    for row_idx, row in data.iterrows():\n",
    "        head, news, _ = get_prompt_by_row(symbol, row)\n",
    "        prev_rows.append((head, news, None))\n",
    "        \n",
    "    prompt = \"\"\n",
    "    for i in range(-len(prev_rows), 0):\n",
    "        prompt += \"\\n\" + prev_rows[i][0]\n",
    "        sampled_news = sample_news(\n",
    "            prev_rows[i][1],\n",
    "            min(5, len(prev_rows[i][1]))\n",
    "        )\n",
    "        if sampled_news:\n",
    "            prompt += \"\\n\".join(sampled_news)\n",
    "        else:\n",
    "            prompt += \"No relative news reported.\"\n",
    "        \n",
    "    period = \"{} to {}\".format(curday, n_weeks_before(curday, -1))\n",
    "    \n",
    "    if with_basics:\n",
    "        basics = get_current_basics(symbol, curday)\n",
    "        basics = \"Some recent basic financials of {}, reported at {}, are presented below:\\n\\n[Basic Financials]:\\n\\n\".format(\n",
    "            symbol, basics['period']) + \"\\n\".join(f\"{k}: {v}\" for k, v in basics.items() if k != 'period')\n",
    "    else:\n",
    "        basics = \"[Basic Financials]:\\n\\nNo basic financial reported.\"\n",
    "\n",
    "    info = company_prompt + '\\n' + prompt + '\\n' + basics\n",
    "    prompt = info + f\"\\n\\nBased on all the information before {curday}, let's first analyze the positive developments and potential concerns for {symbol}. Come up with 2-4 most important factors respectively and keep them concise. Most factors should be inferred from company related news. \" \\\n",
    "        f\"Then make your prediction of the {symbol} stock price movement for next week ({period}). Provide a summary analysis to support your prediction.\"\n",
    "        \n",
    "    return info, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f48aab1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ticker = \"AAPL\"\n",
    "n_weeks = 2\n",
    "curday = get_curday()\n",
    "steps = [n_weeks_before(curday, n) for n in range(n_weeks + 1)][::-1]\n",
    "\n",
    "data = get_stock_data(ticker, steps)\n",
    "\n",
    "data = get_news(ticker, data)\n",
    "\n",
    "data['Basics'] = [json.dumps({})] * len(data)\n",
    "# data = get_basics(ticker, data, always=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bb302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "info, prompt = get_all_prompts_online(ticker, data, curday, False)\n",
    "\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
